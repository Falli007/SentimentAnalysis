{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2vyygPEGognx"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy import displacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVksTCosp8KI",
        "outputId": "50a883a9-dd2b-4d86-e02e-65a295bf3ef6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "-foZOSs1p-JQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt = (\"Given the recent downturn in stocks especially in tech which is likely to persist as yields keep going up, \"\n",
        "       \"I thought it would be prudent to share the risks of investing in ARK ETFs, written up very nicely by \"\n",
        "       \"[The Bear Cave](https://thebearcave.substack.com/p/special-edition-will-ark-invest-blow). The risks comes \"\n",
        "       \"primarily from ARK's illiquid and very large holdings in small cap companies. ARK is forced to sell its \"\n",
        "       \"holdings whenever its liquid ETF gets hit with outflows as is especially the case in market downturns. \"\n",
        "       \"This could force very painful liquidations at unfavorable prices and the ensuing crash goes into a \"\n",
        "       \"positive feedback loop leading into a death spiral enticing even more outflows and predatory shorts.\")"
      ],
      "metadata": {
        "id": "BbfoX_xLrCdx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(txt)\n",
        "\n",
        "displacy.render(doc, style='ent')\n",
        "# displacy.serve(doc, style='ent') if not running in a notebook"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "TrfY8CuLqLmt",
        "outputId": "b7318e4e-9e0f-4245-e88d-04398572268f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Given the recent downturn in stocks especially in tech which is likely to persist as yields keep going up, I thought it would be prudent to share the risks of investing in \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    ARK\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " ETFs, written up very nicely by [\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    The Bear Cave](https://thebearcave.substack.com/p\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "/special-edition-will-ark-invest-blow). The risks comes primarily from \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    ARK\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "'s illiquid and very large holdings in small cap companies. \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    ARK\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " is forced to sell its holdings whenever its liquid \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    ETF\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " gets hit with outflows as is especially the case in market downturns. This could force very painful liquidations at unfavorable prices and the ensuing crash goes into a positive feedback loop leading into a death spiral enticing even more outflows and predatory shorts.</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Immediately we're able to produce not perfect, but pretty good NER. We are using the en_core_web_sm model - en referring to English and sm small.\n",
        "\n",
        "The model is accurately identifying ARK as an organization. It does also classify ETF (exchange traded fund) as an organization, which is not the case (an ETF is a grouping of securities on the markets), but it's easy to see why this is being classified as one. The other tag we can see is WORK_OF_ART, it isn't inherently clear what exactly this means, so we can get more information using spacy.explain:"
      ],
      "metadata": {
        "id": "qqTtOjYkrYyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.explain('WORK_OF_ART')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "HlD6IiQ1rM7P",
        "outputId": "82a721e4-bdf6-46cf-c569-12cfb5b2aa64"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Titles of books, songs, etc.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.explain('ORG') #organisation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "pRv0iIsbxO79",
        "outputId": "37a899e4-a0f9-48ba-80c2-c0eb4e885eb5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Companies, agencies, institutions, etc.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we can see that this description fits well to the tagged item, which refers to an article (although not quite a book).\n",
        "\n",
        "We have a visual output from our tagged text, but this won't be particularly useful programatically. What we need is a way to extract the relevant tags (the organizations) from our text. To do that we can use doc.ents which will return a list of all identified entities.\n",
        "\n",
        "Each item in this entity list contains two attributes that we are interested in, label_ and text:"
      ],
      "metadata": {
        "id": "Mnob6fRVvH5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for entity in doc.ents:\n",
        "    print(f\"{entity.label_}: {entity.text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMIpB3StuSvU",
        "outputId": "ab0211c3-6331-424a-848b-36d9a63b4fdb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPE: ARK\n",
            "ORG: The Bear Cave](https://thebearcave.substack.com/p\n",
            "ORG: ARK\n",
            "ORG: ARK\n",
            "ORG: ETF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we need to filter out any entities that are not ORG entities, and append those remaining ORGs to an organization list:"
      ],
      "metadata": {
        "id": "xRgTmfyVv3UV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize our list\n",
        "org_list = []\n",
        "\n",
        "for entity in doc.ents:\n",
        "    # if label_ is ORG, we append text, otherwise ignore\n",
        "    if entity.label_ == 'ORG':\n",
        "        org_list.append(entity.text)\n",
        "\n",
        "org_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BK4FaNs9vwTw",
        "outputId": "c66ac209-1701-4c3b-a6a8-f4041b5c2e53"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The Bear Cave](https://thebearcave.substack.com/p', 'ARK', 'ARK', 'ETF']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# I don't need to see 'ARK' three times, so I use set() to remove duplicates, and then convert back to list\n",
        "org_list = list(set(org_list))\n",
        "\n",
        "org_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2GcqSNWwEUx",
        "outputId": "4d47bad2-0d9d-4334-a446-7da3b397edcb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ARK', 'The Bear Cave](https://thebearcave.substack.com/p', 'ETF']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"Apple is looking at buying U.K. startup for $1 billion\""
      ],
      "metadata": {
        "id": "ipoHs8NqyDQw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting Reddit Data\n",
        "There are two options for extracting data from Reddit:\n",
        "\n",
        "The requests library, which will allow us to interface directly with the Reddit API.\n",
        "\n",
        "The PRAW library, which is a wrapper library that adds an extra layer of abstraction in accessing the Reddit API.\n",
        "\n",
        "Here we will cover the first option, using the requests library to interface directly with the API.\n",
        "\n",
        "The final extraction script will look like this:"
      ],
      "metadata": {
        "id": "KMwH7n8qzZXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "class Reddit:\n",
        "    def __init__(self, client_id, secret_token, username, password):\n",
        "        # first create authentication object\n",
        "        auth = requests.auth.HTTPBasicAuth(client_id, secret_token)\n",
        "        # build login dictionary\n",
        "        login = {'grant_type': 'password',\n",
        "                 'username': username,\n",
        "                 'password': password}\n",
        "        # setup header info (incl description of API)\n",
        "        headers = {'User-Agent': 'MyBot/0.0.1'}\n",
        "        # send request for OAuth token\n",
        "        res = requests.post(f'https://www.reddit.com/api/v1/access_token',\n",
        "                            auth=auth, data=login, headers=headers)\n",
        "        # pull auth bearer token from response\n",
        "        token = res.json()['access_token']\n",
        "        # add authorization to headers dictionary\n",
        "        headers['Authorization'] = f'bearer {token}'\n",
        "        # add headers dict to internal attributes\n",
        "        self.headers = headers\n",
        "        # and api\n",
        "        self.api = 'https://oauth.reddit.com'\n",
        "\n",
        "    def get_new(self, subreddit, iters):\n",
        "        # initialize dataframe to store data\n",
        "        df = pd.DataFrame()\n",
        "        # initialize parameters dictionary\n",
        "        params = {'limit': 100}\n",
        "        # iterate through several times to make sure we get all the data available\n",
        "        for i in range(iters):\n",
        "            # make request\n",
        "            res = requests.get(f'{self.api}/r/{subreddit}/new',\n",
        "                               headers=self.headers,\n",
        "                               params=params)\n",
        "            # check that we returned something (if not we reached end)\n",
        "            if len(res.json()['data']['children']) == 0:\n",
        "                print('No more found')\n",
        "                return df\n",
        "            # iterate through each thread recieved\n",
        "            for thread in res.json()['data']['children']:\n",
        "                # add info to dataframe\n",
        "                df = df.append({\n",
        "                    'id': thread['data']['name'],\n",
        "                    'created_utc': int(thread['data']['created_utc']),\n",
        "                    'subreddit': thread['data']['subreddit'],\n",
        "                    'title': thread['data']['title'],\n",
        "                    'selftext': thread['data']['selftext'],\n",
        "                    'upvote_ratio': thread['data']['upvote_ratio'],\n",
        "                    'ups': thread['data']['ups'],\n",
        "                    'downs': thread['data']['downs'],\n",
        "                    'score': thread['data']['score']\n",
        "                }, ignore_index=True)\n",
        "            # get earliest ID\n",
        "            earliest = df['id'].iloc[len(df)-1]\n",
        "            # add earliest ID to params\n",
        "            params['after'] = earliest\n",
        "        return df\n"
      ],
      "metadata": {
        "id": "V_oKtA5Fzasa"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SUB = 'investing'\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "0r0UQqVqzxyF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CLIENT_ID = '<CLIENT_ID>'\n",
        "SECRET_TOKEN = '<SECRET_TOKEN>'"
      ],
      "metadata": {
        "id": "Qt24xe1Cz4aS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "USER = '<USER>'\n",
        "PWD = '<PASSWORD>'"
      ],
      "metadata": {
        "id": "ezpDrFlzz49h"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reddit = Reddit(CLIENT_ID, SECRET_TOKEN, USER, PWD)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "5K-it572z_M-",
        "outputId": "6fa1f7c3-675f-4926-e91e-e0e8c8d162e1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'access_token'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-1de8896159bc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreddit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReddit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCLIENT_ID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSECRET_TOKEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUSER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPWD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-c4c0b09ac36e>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, client_id, secret_token, username, password)\u001b[0m\n\u001b[1;32m     17\u001b[0m                             auth=auth, data=login, headers=headers)\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# pull auth bearer token from response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'access_token'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;31m# add authorization to headers dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mheaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Authorization'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'bearer {token}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'access_token'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = reddit.get_new(SUB, 20)\n"
      ],
      "metadata": {
        "id": "vN8lkp7K0H8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No more found"
      ],
      "metadata": {
        "id": "-SoZLtSe0Me7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.replace({'|': ''}, regex=True)\n",
        "data.to_csv(f'./data/reddit_{SUB}.csv', sep='|', index=False)"
      ],
      "metadata": {
        "id": "bnUl2ap20OsG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}